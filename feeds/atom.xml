<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://florianbrand.de/</id>
  <title>Florian Brand</title>
  <updated>2025-03-09T21:30:23.542936+00:00</updated>
  <author>
    <name>Florian Brand</name>
    <email>privat@florianbrand.de</email>
    <uri>https://florianbrand.de</uri>
  </author>
  <link href="https://florianbrand.de" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <rights>All rights reserved 2025, Florian Brand</rights>
  <subtitle>Florian Brand</subtitle>
  <entry>
    <id>https://florianbrand.de/posts/fasthtml-intro</id>
    <title>How FastHTML sparked my joy in web development</title>
    <updated>2025-03-09T21:30:23.591757+00:00</updated>
    <author>
      <name>Florian Brand</name>
      <email>privat@florianbrand.de</email>
    </author>
    <content type="CDATA"><![CDATA[<h1 id="how-fasthtml-sparked-my-joy-in-web-development">How FastHTML sparked my joy in web development</h1>
<p>I've been wanting to create a personal site for a while now. 
Nowadays, there are basically two options: Either build a full-scale web project with JavaScript/TypeScript and React/NextJS or using a static site generator like Jekyll or Hugo. 
The first option can become overwhelmingly complex, especially for someone like me, who is not a web developer.
It feels like everyone prefers (and recommends) the most sophisticated setup to accomplish something, even if it should be rather simple in principle.
For instance, you should use Typescript over JavaScript and transpile the code, use a huge framework like NextJS, bun over npm, and so on.
Even something as simple as styling a site is overloaded with <a href="https://nextjs.org/docs/app/building-your-application/styling">so many options</a> of varying complexity, that it is hard as an outsider to get started.</p>
<p>Naturally, this lead me to the latter, with me trying out <a href="https://gohugo.io/">Hugo</a>, which is a static site generator.
That means that you install the hugo binary, write your content in markdown, and then run <code>hugo</code> to generate the site, resulting in plain HTML files.
The setup is easy and in mere minutes, a static site is created and running.
Hugo has a lot of third-party <a href="https://themes.gohugo.io/">themes</a>, which can be easily installed and used.
However, Hugo requires you to conform to their setup, minimizing the flexibility of the site.
Furthermore, debugging errors is hard, as a lot of things are abstracted away from the user.</p>
<p>As luck would have it, just as I was looking for alternatives to these problems, the Answer.ai team around Jeremy Howard released <a href="https://fastht.ml/">FastHTML</a>, which is described as "a new way to create modern interactive web apps".
FastHTML is built on top of <a href="https://www.starlette.io/">Starlette</a> and <a href="https://www.uvicorn.org/">uvicorn</a>, meaning people which are familiar with FastAPI will feel right at home.
Aside from the technology, using FastHTML feels like using PHP, as you write the HTML tags using Python, with each tag being a simple function:</p>
<pre class="codehilite"><code class="language-python">from fasthtml.common import *
page = Html(
    Head(Title('Some page')),
    Body(Div('Some text, ', A('A link', href='https://example.com'), 
             cls='myclass')))
print(to_xml(page))
</code></pre>

<p>This simple example is easily readable and understandable, which results in this HTML code:</p>
<pre class="codehilite"><code class="language-html">&lt;!doctype html&gt;

&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Some page&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class=&quot;myclass&quot;&gt;
Some text, 
      &lt;a href=&quot;https://example.com&quot;&gt;A link&lt;/a&gt;
    &lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>FastHTML is more than just a simple HTML generator, as it also offers a full-blown webserver with routing, making it pretty easy to start a simple site with very little code and then scale it up as needed.
For interactivity, they support <a href="https://htmx.org/">htmx</a>, which, as an outsider to the JS world, feels more intuitive than other frameworks, as it adds attributes to the HTML tags for interactivity.
This site is built with FastHTML, with the whole code being available <a href="https://github.com/Xceron/florianbrand.de">on GitHub</a>.</p>
<p>Now that we've covered the basics, let me share my personal experiences with this new framework.</p>
<h2 id="initial-hurdles">Initial Hurdles</h2>
<p>Being a new framework, FastHTML naturally has less (third-party) resources to learn the ins and outs of the framework.
However, the community is growing fast, with multiple projects and examples showing the usage of the library few days after the initial release.
I have compiled a small list <a href="#resources">down below</a>.</p>
<p>While coding in FastHTML, I ran into a few quirks. 
One that stood out was a bug in PyCharm related to the live reload feature. 
FastHTML normally lets you see changes instantly when you save a file, which is very convenient for local development. 
But in PyCharm, this feature does not work correctly, as changes either don't apply or take ages to show up. 
Turns out it's a <a href="https://youtrack.jetbrains.com/issue/PY-60962/os.killpid-signal.CRTLCEVENT-ignored-when-running-python-program-from-Pycharm">known bug</a>, which is not yet fixed.
A workaround is to open the run configuration and enabling the option <code>Emulate terminal in output console</code> (hidden under <code>Modify options</code>).<sup id="fnref:pycharm"><a class="footnote-ref" href="#fn:pycharm">1</a></sup>
This will, however, make the terminal output harder to read, as the ANSI escape sequences, which are used for colored output, are displayed and not rendered.
<img alt="img" src="img/fasthtml-intro-ansi.png" /></p>
<p>Speaking of editors: At first glance, FastHTML code looks unconventional in a Python editor.
This unconventional look is due to two main factors. First, the recommended way to import most of FastHTMLs' functionality with <code>from fasthtml.common import *</code>. Second, the fact that (nested) HTML tags take up a lot of space rather fast means that they take up a lot of visual space.
The first issue results in linters like pylance or ruff not liking the code, as PEP8 recommends to import only the needed functions<sup id="fnref:imports"><a class="footnote-ref" href="#fn:imports">2</a></sup>.
To illustrate the second issue, here is some code of the FastHTML landing page:
<img alt="img" src="img/fasthtml-intro-code.png" /></p>
<p>While it is valid code, it <em>feels</em> weird to see inside a Python file and gets some time to get used to.</p>
<p>Overall, these are not really issues in the power of the AnswerAI team, which underlines how great the framework already is and the overall potential of FastHTML.</p>
<h2 id="becoming-productive-with-fasthtml">Becoming productive with FastHTML</h2>
<p>Admittedly, I needed some time to get started with FastHTML.
This is not a problem of the framework, as my capabilities as a web developer are rather limited and I am a bit rusty when it comes to writing "pure" HTML/CSS.
But after it clicked, it truly is pleasant to use and you can iterate very fast.
For the frontend/HTML, my workflow is making a screenshot of the current page, drawing on top of it with changes I want to apply, and then giving this screenshot with the generated HTML and CSS file to Claude.
Claude then generates the changes by writing HTML, which I then convert with <a href="https://h2f.answer.ai/">h2f.answer.ai</a> and add to my Python file.
Therefore, it is not really a problem that LLMs don't have knowledge of FastHTML, because it is so close to just HTML.
This also applies to htmx, which has been around for over 10 years and is a well-established framework.
Consequently, the Python code of FastHTML pages is easily readable, which makes learning even easier once you know the basics.
For the backend, FastHTML builds on top of Starlette and uvicorn, which are both well established with a lot of resources available.
For instance, when I needed to redirect to the home page on a 404 error, I simply provided Claude with the context that FastHTML uses these frameworks. Claude then generated code using exception handlers and a RedirectResponse (<a href="https://github.com/Xceron/florianbrand.de/blob/c273a5ea6e181891685d6d06336edbc5c0a5b93e/main.py#L21">Code</a>).</p>
<p>Beyond the coding aspects, FastHTML impresses with its minimal resource requirements.
I run this site on a small VPS (2 vCores, 2GB RAM, more context in the <a href="#deployment">deployment section</a>).
When idle, the usage looks like this<sup id="fnref:usage"><a class="footnote-ref" href="#fn:usage">3</a></sup>:
<img alt="img" src="img/fasthtml-intro-lzd.png" /></p>
<p>Of course the idle state of a small site is not impressive.
Hitting the front page of hackernews and handling it without breaking a sweat, however, is:
<img alt="img" src="img/fasthtml-intro-jh-tweet.png" /></p>
<h2 id="resources">Resources</h2>
<p>There are already some amazing resources to get started with FastHTML:</p>
<ul>
<li>Official materials<ul>
<li>The comprehensive <a href="https://docs.fastht.ml/">documentation</a></li>
<li>The <a href="https://youtu.be/Auqrm7WFc0I?si=gzU0fe2kcdXwpAX7">introduction video</a> from Jeremy Howard, offering a general overview</li>
<li>An <a href="https://youtu.be/WuipZMUch18">interview</a> with the HTMX creator Carson Gross, offering a look into the interactivity provided by HTMX and their vision of the framework(s)</li>
</ul>
</li>
<li>Tutorial content<ul>
<li>CalmCode's <a href="https://youtu.be/4En57Zw6gU4?si=ehYOIhzdYj0itouf">video tutorial</a> on building interactive charts with matplotlib and htmx</li>
<li>A <a href="https://gist.github.com/jph00/26200368915f6aabb450eaf33a03c3e8">heavily commented example</a>, perfect for providing context to a LLM</li>
</ul>
</li>
<li>Tools and examples<ul>
<li><a href="https://h2f.answer.ai/">h2f.answer.ai</a> for converting HTML tags to FastHTML functions</li>
<li><a href="https://fasthtml.gallery/">FastHTML.Gallery</a> for minimal implementations of web components</li>
<li>Code of several real-life examples, including the <a href="https://github.com/AnswerDotAI/home-fasthtml">FastHTML landing page</a>, the <a href="https://github.com/AnswerDotAI/fh-about">About FastHTML website</a>, (which uses <a href="https://github.com/AnswerDotAI/fh-bootstrap">fh-bootstrap</a>), and this very <a href="https://github.com/Xceron/florianbrand.de">site</a>!</li>
</ul>
</li>
<li>Community support<ul>
<li>The #fasthtml channel on the <a href="https://discord.gg/qcXvcxMhdP">FastAI Discord</a></li>
</ul>
</li>
</ul>
<h2 id="deployment">Deployment</h2>
<p>I've deployed the site on a VPS from netcup. For just €3.99 per month, I get 2 vCores, 2GB RAM, and a 64GB SSD (<a href="https://www.netcup.eu/bestellen/produkt.php?produkt=3899">link</a>). 
This is a fantastic deal, especially when compared to cloud providers like AWS. 
These services often charge 3-5 times more for similar resources and can be more complex to set up. Platform-as-a-Service (PaaS) options like Railway, Heroku, and Vercel are easier-to-use alternatives, but they can become costly if your site experiences high traffic.
Personally, I find easier to manage and use a VPS, which I can ssh into to set up my things, such as this blog, compared to a service which tries to abstract this away from me. </p>
<p>FastHTML offers various <a href="https://docs.fastht.ml/tutorials/deployment.html">templates</a> to deploy the site for popular providers. As there is no guide for a generic, self-hosted setup, so here is my setup:</p>
<ul>
<li>A (public) git repository with the code of the server, which is available <a href="https://github.com/Xceron/florianbrand.de">here</a>. As the site only consists of (Fast)HTML, CSS and markdown files with a very basic routing scheme, it is okay to have the code public. The default should be a private repository. </li>
<li>Docker with docker compose to build and run FastHTML and the associated server, which can be accessed <a href="https://github.com/Xceron/florianbrand.de/blob/main/Dockerfile">here</a><sup id="fnref:docker"><a class="footnote-ref" href="#fn:docker">4</a></sup>. While it is not necessary to use Docker, it makes the setup easier and more reproducible between the local development machine and the VPS.</li>
<li><a href="https://caddyserver.com/">Caddy</a>, which is an alternative to nginx and acts as a reverse proxy and web server. It is easier to set up and configure than nginx, as it has automatic HTTPS and a simple configuration file.</li>
</ul>
<p>The process to deploy the site is as follows:</p>
<ol>
<li>ssh into the VPS server</li>
<li>Install Docker and Docker Compose</li>
<li>Clone the repository of the site</li>
<li>Create a file named <code>Caddyfile</code> with the configuration for the site</li>
<li>Create a file named <code>docker-compose.yml</code> with the configuration for the FastHTML server and the Caddy server</li>
<li>Run <code>docker-compose up --build -d</code> to build and run the FastHTML server and the Caddy server</li>
</ol>
<p>The <code>Caddyfile</code> for an exemplary page at <code>yourpage.com</code> would look like this:</p>
<pre class="codehilite"><code class="language-Caddy">YOURPAGE.com, www.YOURPAGE.com {
    reverse_proxy fasthtml-app:5001
}
</code></pre>

<p>The <code>docker-compose.yml</code> file would look like this:</p>
<pre class="codehilite"><code class="language-yaml">services:
  caddy:
    image: caddy:latest
    ports:
      - &quot;80:80&quot;
      - &quot;443:443&quot;
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile

  fasthtml-app:
    build:
      context: ./&lt;PATH TO THE LOCAL REPOSITORY&gt;
      dockerfile: Dockerfile
    container_name: fasthtml-app
</code></pre>

<p>This is everything needed for the deployment. The site should be available at <code>yourpage.com</code> and <code>www.yourpage.com</code> with automatic HTTPS being set up by Caddy.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>For me, FastHTML has filled a desire I had for a long time: A simple way to create websites, without requiring me to learn the complexity which is modern web development.
I never thought I'd be using Python like PHP, but it is a welcome change and being able to tap into the vast ecosystem of Python libraries is a huge plus.
For now, I don't think FastHTML will replace (my usage of) gradio and streamlit, which make it easy to create good-looking MVPs in a mere minutes with few lines of code.
This could change, however, if the community grows and makes components easily available, which would make it easier to create more complex sites with less code.
In the community discord, there are people from different backgrounds: Data scientists, which want to create beautiful dashboards, web developers, which want to have a simpler way to create websites,  Django powerusers, which know everything about Python web development and server, and many more.
I will watch the development of FastHTML closely and can't wait to see where the framework will be in a year or two.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:pycharm">
<p>Alternatively, you can use VSCode/Cursor or run it in your terminal, which all work perfectly fine with live reloading, also on Windows.&#160;<a class="footnote-backref" href="#fnref:pycharm" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:imports">
<p>There is an <a href="https://github.com/AnswerDotAI/fasthtml/blob/main/examples/pep8_app.py">example</a> of a PEP8 conform app.py with <code>from fasthtml import common as fh</code>, which then results in a lot of <code>fh.</code> in the code, making it less readable. Alternatively, you could also import every used HTML tag, but this results in ~20 lines of imports rather quickly.&#160;<a class="footnote-backref" href="#fnref:imports" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:usage">
<p>Screenshot done with the amazing <a href="https://github.com/jesseduffield/lazydocker">lazydocker</a> tool&#160;<a class="footnote-backref" href="#fnref:usage" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:docker">
<p>The Dockerfile uses <a href="https://github.com/astral-sh/uv">uv</a> to install the python dependencies instead of pip. uv is much faster than pip, but is not part of the Docker images, so it has to be installed first.&#160;<a class="footnote-backref" href="#fnref:docker" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>]]></content>
    <link href="https://florianbrand.de/posts/fasthtml-intro"/>
    <summary>Buiding a personal site with FastHTML</summary>
    <category term="web"/>
    <category term="fasthtml"/>
    <contributor>
      <name>Florian Brand</name>
      <email>privat@florianbrand.de</email>
    </contributor>
    <published>2024-08-04T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://florianbrand.de/posts/uv-intro</id>
    <title>Sane Python dependency management with uv</title>
    <updated>2025-03-09T21:30:23.584819+00:00</updated>
    <author>
      <name>Florian Brand</name>
      <email>privat@florianbrand.de</email>
    </author>
    <content type="CDATA"><![CDATA[<h1 id="sane-python-dependency-management-with-uv">Sane Python dependency management with uv</h1>
<p>Astral, the company behind the fast and beloved python linter <a href="https://astral.sh/ruff">ruff</a> has been working on <a href="https://github.com/astral-sh/uv">uv</a> for some time now, with the ambitious goal to bring the philosophy of cargo, rusts package manager, to the python ecosystem.
Personally, I have never been a fan of alternative package managers and tools like conda, poetry, pipx and others, as they require you to install a whole tool for a certain use case, addressing only a single pain point.
However, uv is different, and I've been using it for some months now as a replacement for pip.
uv offers a very noticeable speedup for installing packages, even on small projects, while using the same commands as pip.
Some projects are already switching their pipelines and report big speedups, e.g. streamlit reducing their load time up to <a href="https://blog.streamlit.io/python-pip-vs-astral-uv/">55%</a> or Google having a <a href="https://x.com/cgarciae88/status/1826589523020595546">2-5x speedup</a> on their CI/CD pipelines for a JAX library.</p>
<p>With <a href="https://astral.sh/blog/uv-unified-python-packaging">uv 0.3.0</a>, Astral is targeting more than just pip and want to provide a unified interface for all python packaging needs, from CLI tools to complicated projects spanning multiple packages. 
This post is meant as a small introduction into <code>uv</code> to showcase some of the features, everything else can be found in the <a href="https://docs.astral.sh/uv/">documentation</a>.</p>
<h2 id="installation">Installation</h2>
<p>Installing uv is pretty easy:</p>
<pre class="codehilite"><code class="language-bash">curl -LsSf https://astral.sh/uv/install.sh | sh
</code></pre>

<p>On Windows, it can be installed with PowerShell as follows:</p>
<pre class="codehilite"><code class="language-powershell">powershell -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;
</code></pre>

<h2 id="usage">Usage</h2>
<h3 id="pip-commands">pip commands</h3>
<p>The easiest way to get started with uv is to use the <a href="https://docs.astral.sh/uv/pip/">pip interface</a>.
To do this, simply replace any <code>pip</code> command with <code>uv pip</code>, e.g. <code>uv pip install requests</code> instead of <code>pip install requests</code> or <code>uv pip freeze</code> instead of <code>pip freeze</code>.
This way, you gain the speedup of uv without having to learn any new commands or other tools.
You can also use uv with <a href="https://docs.astral.sh/uv/pip/environments/#discovery-of-python-environments">existing venvs/projects</a>, making the switch to uv very easy. </p>
<h3 id="uv-commands">uv commands</h3>
<p>Aside from the pip interface, uv also offers its own commands, which are more powerful and can be used to manage your python projects.
For projects, the following commands are of importance when working with python projects:</p>
<table>
<thead>
<tr>
<th>uv command</th>
<th>pip command(s)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>uv init &lt;name&gt;</code></td>
<td>N/A</td>
<td>creates a new, empty project according to the <code>pyproject.toml</code> specification</td>
</tr>
<tr>
<td><code>uv venv</code></td>
<td><code>python -m venv .venv</code></td>
<td></td>
</tr>
<tr>
<td><code>uv add &lt;package&gt;</code></td>
<td><code>pip install &lt;package&gt;</code></td>
<td>uv also adds it to the pyproject.toml, similar to <code>poetry add &lt;package&gt;</code>, which pip does not</td>
</tr>
<tr>
<td><code>uv remove &lt;package&gt;</code></td>
<td><code>pip uninstall &lt;package&gt; &amp;&amp; pip freeze &gt; requirements.txt</code></td>
<td></td>
</tr>
<tr>
<td><code>uv sync</code></td>
<td><code>pip freeze \| xargs pip uninstall -y &amp;&amp; pip install -r requirements.txt</code></td>
<td>uv also creates a venv if it doesn't exist</td>
</tr>
<tr>
<td><code>uv run &lt;python_file&gt;</code></td>
<td><code>source .venv/activate &amp;&amp; python &lt;python_file&gt;</code></td>
<td>uv can also work with inline dependencies, removing the need for a venv directory (<a href="https://docs.astral.sh/uv/reference/cli/#uv-run">Docs</a>)</td>
</tr>
</tbody>
</table>
<p>The best thing about uv: it tries to fix errors itself instead of throwing them back to you.
As an example, if you just have a <code>pyproject.toml</code> with a python version you have not installed, uv will install if for you[^envs], then create a venv and install the dependencies.</p>
<h2 id="the-uv-ecosystem">The uv ecosystem</h2>
<p>As mentioned in the introduction, uv tries to capture the whole ecosystem, not (just) the management of python projects like poetry.</p>
<p>uv categorizes between <a href="https://docs.astral.sh/uv/concepts/tools/">tools</a>, <a href="https://docs.astral.sh/uv/guides/scripts/">scripts</a>, <a href="https://docs.astral.sh/uv/concepts/projects/">projects</a> and <a href="https://docs.astral.sh/uv/concepts/workspaces/">workspaces</a>:</p>
<p><img alt="" src="img/uv-overview.svg" /></p>
<h3 id="tools">Tools</h3>
<p>In uv, tools are meant for CLI tools, such as ruff or <a href="https://github.com/simonw/llm">llm</a>. 
If you run <code>uvx &lt;tool&gt;</code>, uv will automatically install the tool if you don't have it installed yet; else it will run the tool.
uv will create an isolated environment for the tool.
Changes to the tool are preserved - if you store your API key(s) for the <code>llm</code> tool, running <code>uvx llm</code> another time will still have the API key(s) stored.
The tool command is meant as a replacement for <a href="https://github.com/pypa/pipx">pipx</a>.</p>
<h3 id="scripts">Scripts</h3>
<p>In uv, scripts mean single python files without a <code>pyproject.toml</code> or other project files.
You can use dependencies either with <code>uv run --with &lt;dependencies&gt; &lt;script&gt;</code>, e.g. <code>uv run --with requests script.py</code>, or by including a special comment at the start of the file:
<img alt="uv-script-run.png" src="img/uv-script-run.png" /></p>
<p>In this example, the first lines indicate the python version and the dependencies needed to run the script. 
Just like other commands, uv will automatically detect if you miss the python version or dependencies and install them for you.
I expect this syntax, which is a new <a href="https://packaging.python.org/en/latest/specifications/inline-script-metadata/#inline-script-metadata">standard</a>, to be more widespread in the near feature, as it does not clutter your global python installation nor require the explicit setup of a venv just for a single script.</p>
<h3 id="projects-and-workspaces">Projects and Workspaces</h3>
<p>For projects, uv uses and expects a <code>pyproject.toml</code> file.
When you install dependencies, uv will create a <code>uv.lock</code> file, which is the only file  not adhering to a python standard.
It contains the exact versions of all the dependencies and is a cross-platform lock file, which should not be edited by hand.</p>
<p>Workspaces are meant for more complex projects, which span multiple packages.
The documentation for workspaces can be found <a href="https://docs.astral.sh/uv/concepts/workspaces/">here</a>.</p>
<h2 id="docker">Docker</h2>
<p>The easiest way to use uv in Docker images is to copy the official Docker image:</p>
<pre class="codehilite"><code class="language-Dockerfile">FROM python:3.12-slim-bookworm
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv
</code></pre>

<p>Then, you can copy the content of your python project and run <code>uv sync</code>.
As an example, this is the <a href="https://github.com/Xceron/florianbrand.de/blob/main/Dockerfile">current Dockerfile</a> for this website:</p>
<pre class="codehilite"><code class="language-Dockerfile">FROM python:3.12-slim
COPY --from=ghcr.io/astral-sh/uv:0.3.1 /uv /bin/uv

ADD . /app
WORKDIR /app

RUN uv sync

CMD [&quot;uv&quot;, &quot;run&quot;, &quot;main.py&quot;]
</code></pre>

<h2 id="conclusion">Conclusion</h2>
<p><img alt="uv-horse.jpg" src="img/uv-horse.jpg" /></p>
<p>The team behind astral made a genius move by luring in users with a familiar pattern to replace pip and get speedups, just to offer them a whole suite to replace all needs a python user might have.
I think it's a great tool and, similar to ruff, will see widespread adoption in a short time by the python community.</p>]]></content>
    <link href="https://florianbrand.de/posts/uv-intro"/>
    <summary>An overview of uv, the fast package manager</summary>
    <category term="tools"/>
    <category term="til"/>
    <contributor>
      <name>Florian Brand</name>
      <email>privat@florianbrand.de</email>
    </contributor>
    <published>2024-08-23T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://florianbrand.de/posts/llm-programmer-tutorial</id>
    <title>A Guide to LLMs for Programmers</title>
    <updated>2025-03-09T21:30:23.571404+00:00</updated>
    <author>
      <name>Florian Brand</name>
      <email>privat@florianbrand.de</email>
    </author>
    <content type="CDATA"><![CDATA[<h1 id="a-guide-to-llms-for-programmers">A Guide to LLMs for Programmers</h1>
<p>Although LLMs are all the rage these days, I found that a surprising amount of developers is still dismissing them and their potential. 
Part of this might be due to the limited amount of information aimed at developers, as most tutorials and guides are aimed at the NLP community or developers building LLM applications. This post provides an overview to the world of LLMs, the applications to access them and some prompting tips aimed at people who code. Most of these are based on my personal experiences from the past years of using them.</p>
<h2 id="a-short-technical-introduction-to-llms">A short technical Introduction to LLMs</h2>
<p>While there are a lot of technical details differentiating LLMs, most of them are not that interesting from a users perspective or are intentionally withheld by the makers of the models to protect their IP. For programmers, two aspects are important: The <strong>context window size</strong> and the <strong>knowledge cutoff</strong> date.</p>
<p>The context window is the amount of tokens (words/parts of words) a model can process at a time, with one token representing roughly 3-4 characters. This is similar to the RAM of computers: Text which is bigger than the context window simply cannot be processed.</p>
<p>To calculate the amount of tokens for a given text, <a href="https://github.com/simonw/ttok">ttok</a> is a simple to use utility for all OpenAI models, but every model uses their own tokenizer, therefore the actual amount of tokens for the same text differs from model to model. However, modern tokenizers are rather similar to each other in terms of the amount of tokens they produce. As a rule of thumb you can use the tokenizer for <code>GPT-4o</code> from OpenAI (<code>o200k_base</code>) and use this number as an educated guess, the others are usually within 10-20% of that number.</p>
<p>Another aspect mostly overlooked in literature, but crucial for software development is the knowledge cutoff date. This marks the end date of the used training data and, as the name applies, the most recent data the model will have knowledge of. For software development this impacts the latest versions of libraries the model can have knowledge of. If you work with cutting-edge libraries with frequent or recent API changes, you will notice this limitation. As an example, I use <a href="https://pola.rs/">polars</a> over pandas for dealing with all kinds of data, which just recently got its first stable release. Models with an old cutoff date, such as GPT-4o (October 2023 cutoff), are unable to generate valid polars code, and often use pandas syntax or outdated APIs, while models with a more recent cutoff date, such as Claude 3.5 Sonnet (April 2024 cutoff) can generate valid polars code.</p>
<p>Training data of LLMs can be understood as a snapshot of the internet at the time of the knowledge cutoff data. Therefore, LLMs mirror the usage of libraries and APIs, including their most-used versions/iterations at this time. As an example, they will be able to generate better <code>matplotlib</code> code than <code>plotly</code> code, as the former library is more popular. But it also means that they might generate code which is outdated, as the training data contains more old code than new code with the updated API usage.
On the one hand, this reinforces the "winner takes all" aspect of libraries, as switching to an alternative library becomes harder. On the other hand, this means that LLMs can generate code for hard-to-use libraries such as <code>ffmpeg</code>, as it is so prevalent in the training data<sup id="fnref:jq"><a class="footnote-ref" href="#fn:jq">1</a></sup>.</p>
<h2 id="selecting-the-appropriate-llm-for-a-task">Selecting the appropriate LLM for a Task</h2>
<p>While the LLM space is as active as ever with promising LLMs releasing almost every week, the frontier has largely settled among the giants of OpenAI, Anthropic and Google. However, not all models are equal and every model has its own strength.
Currently, these patterns emerged in my daily usage:</p>
<ul>
<li><strong>Claude 3.5 Sonnet</strong> for everyday tasks/coding. It has the most recent knowledge cutoff with April 2024 and a rather big context window of 200k tokens. It is an exceptional strong coding model and I use it for the vast majority of tasks.</li>
<li><strong>o1(-preview)</strong> for the planning of codebases / complex algorithms. As this model is vastly different from other models, I have a section dedicated to it later.</li>
<li><strong>Mistral Large 2</strong> for everything related to the (bash) shell and common commands. While other LLMs often come up with convoluted solutions for rather easy problems (which then don't work most of the time), Mistral Large 2 is a master in the shell and gives just the commands you need. However, for other coding tasks, it is not as good as Sonnet.</li>
<li><strong>Gemini (Flash)</strong> for tedious tasks and data cleanup. It is really fast, is able to handle text, images, audio and video as inputs and it is really cheap<sup id="fnref:gemini"><a class="footnote-ref" href="#fn:gemini">2</a></sup>. <a href="https://simonwillison.net/2024/Oct/17/video-scraping/">This</a> post from Simon Willison is a perfect example of easy data cleaning possible by such models.</li>
</ul>
<p>As an honorable mention, <strong>DeepSeek Coder v2</strong> is a strong, open model which is just behind the frontier<sup id="fnref:deepseek"><a class="footnote-ref" href="#fn:deepseek">3</a></sup>.</p>
<p>The default model in ChatGPT, <strong>GPT-4o</strong>,  is not as strong as the competition, especially when compared against Sonnet. It also has a year-old knowledge cutoff date (October 2023), which makes it even less useful when compared to the model by Anthropic.</p>
<p>While these are my impressions, which seems to be largely the impressions by the general community, they are mostly backed by <em>vibes</em>, i.e., the usage of said models, as benchmarks like <a href="https://lmarena.ai/">LM Arena</a> or <a href="https://evalplus.github.io/leaderboard.html">HumanEval</a> can be gamed and might not catch nuances or your individual use case. That said, some benchmarks to keep an eye on are <a href="https://livecodebench.github.io/leaderboard.html">LiveCodeBench</a>, which use Python coding problems from sites like Leetcode published after a certain date (to avoid testing LLMs on data they have already seen), and <a href="https://scale.com/leaderboard/coding">SEAL</a>, which is a non-public benchmark of various programming problems and languages.</p>
<h2 id="applications-to-access-llms">Applications to access LLMs</h2>
<p>To access the models, there are two different categories of applications relevant for developers: Websites and IDEs/Extensions for existing editors.</p>
<h3 id="websites">Websites</h3>
<p>This is probably the first contact anyone has with LLMs, with the most used site being ChatGPT. Every model maker has its own chat website:</p>
<ul>
<li>Anthropic has <a href="https://claude.ai/">Claude.ai</a> for their models</li>
<li>Mistral has <a href="https://chat.mistral.ai/chat">Le Chat</a></li>
<li>Google has <a href="https://gemini.google.com/">Gemini</a> for consumers and <a href="https://aistudio.google.com/">Google AI Studio</a> aimed at developers</li>
</ul>
<p>Both ChatGPT and Claude offer a premium subscription for $20/mo, which lets you access better models or increase the rate limit of certain features. As we move from simple LLM chat frontends to applications, these sites are starting to offer features to differentiate them to the rest, such as Image generation, Code execution, and Generation of <a href="https://support.anthropic.com/en/articles/9487310-what-are-artifacts-and-how-do-i-use-them">small websites</a>.</p>
<p>However, the models deployed on the websites might offer a different experience compared to access through the API. As an example, the 4o models in ChatGPT are restricted to 32K context length, despite the models being capable of working with 128K context. Also, models have a "system prompt", a prompt which tells them what they can do and how they should behave<sup id="fnref:system-prompt"><a class="footnote-ref" href="#fn:system-prompt">4</a></sup>, which influences how the model behaves and thus could give you a different experience compared to accessing the same model through the API.</p>
<p>If you want to access multiple models for one subscription price at once, you can also use services which aggregate these subscriptions, such as:</p>
<ul>
<li><a href="https://kagi.com/pricing">Kagi</a>, which offers it alongside their search engine in the $25/mo tier</li>
<li><a href="https://poe.com/">Poe</a>, which offers access to all LLMs and a lot of image and video generation models for $20/mo</li>
</ul>
<p>These services access the models through the respective APIs, but may set their own restrictions and system prompts.</p>
<h3 id="ides-extensions">IDEs &amp; Extensions</h3>
<p>As the most development happens inside IDEs and editors, it comes to no surprise that a lot of different extensions are available to boost the productivity of coders. The oldest feature, which most developers should be familiar with, is the full line completion offered by extensions like GitHub Copilot, where specialized models try to complete the current line/function and offers suggestions. In my personal usage, I found both Copilot and the local models in the <a href="https://plugins.jetbrains.com/plugin/14823-full-line-code-completion">JetBrains suite</a> rather lacking, with suggestions being rather subpar or unreliable. This might change with the introduction of new models; GitHub Copilot will offer access to <a href="https://github.blog/news-insights/product-news/bringing-developer-choice-to-copilot/">different models, including Sonnet</a>, while JetBrains has developed their own model, <a href="https://blog.jetbrains.com/blog/2024/10/22/introducing-mellum-jetbrains-new-llm-built-for-developers/">Mellum</a>. At the moment, I use <a href="https://supermaven.com/">Supermaven</a> in my IDEs, which is way faster than Copilot, while being <em>accurate enough</em> to be a help and not a distraction.</p>
<p>All mentioned extensions also offer a chat window inside the respective IDEs, which allow you to use the leading models inside the IDE and are more convenient than using the chat sites directly. A notable exception is <a href="https://www.cursor.com/">Cursor</a>, which is a fork of VSCode and offers a lot of functionality to access LLMs for refactoring or adding functions. The AI functions are more deeply integrated into the editor compared to other solutions, so it has a bit of a learning code. I found the <a href="https://www.arguingwithalgorithms.com/posts/cursor-review.html">post</a> from Tom Yedwab a good overview of Cursors' functions.</p>
<p>There are also tools for the command line, such as <a href="https://aider.chat/">aider</a>, which uses your API keys to edit files directly or <a href="https://github.com/simonw/llm">llm</a>, which allows the usage of LLMs directly in the shell. The latter is especially powerful when used with other utilities, such as <a href="https://simonwillison.net/2024/Oct/27/llm-jq/">jq</a>. As <code>llm</code> is written in Python, it can be used as a tool in uv, which I outlined in <a href="https://florianbrand.de/posts/uv-intro">my post</a> with <code>uv tool install llm</code>.</p>
<h2 id="prompting-methods-for-llms">Prompting Methods for LLMs</h2>
<p>Like mentioned previously, I found that most online tutorials / courses for prompting are aimed at people which developed AI applications and use LLMs as part of the software or scientists which want to leverage different techniques to get better results for the problems they are targeting and thus are not really applicable to the everyday problems programmers face. Here are some of the methods and tips I found the most useful when using LLMs for coding.</p>
<h3 id="be-specific-and-clear">Be specific and clear</h3>
<p>This is probably the most important thing when prompting LLMs. You get better responses by being (overly) specific to what you want in what way. Some questions to ask yourself:</p>
<ul>
<li>What should the code to be generated accomplish?</li>
<li>What is my input, what are the types of the variables? Can you provide some representative inputs? The same applies to outputs.</li>
<li>What is the coding style I want? Should it be object-oriented or rather functional? Should it follow PEP strictly? Do I want type hints?</li>
</ul>
<p>Also, you should describe <em>how</em> the problem should be solved. This can be done with generic instructions, like the following<sup id="fnref:espanso"><a class="footnote-ref" href="#fn:espanso">5</a></sup>:</p>
<pre class="codehilite"><code>&lt;YOUR PROBLEM DESCRIPTION HERE&gt;

First, I want you to think about the requirements of the function.
Then, plan out the steps you need to take to solve the problem.
Finally, write the code using clean, pythonic code by leveraging the language's latest features.
</code></pre>

<p>This is known as <em>(zero-shot) chain-of-thought prompting</em>, with the most "implementations" using "Let's think step by step." instead of the prompt above. Here is an excerpt of Claude's system prompt (emphasis mine):</p>
<pre class="codehilite"><code>When presented with a math problem, logic problem, or other problem benefiting from systematic thinking,  Claude **thinks through it step by step** before giving its final answer.
</code></pre>

<p>This yields better results compared to just asking for a solution. If you can be more specific about the problems and specific steps to solve the problem, this improves the quality of the solution even more. In general, you want <em>longer</em> outputs from the model for most coding problems. Problems which require not a lot of "thinking" can be solved without such long prompts. Examples for such problems are the formatting of code, adding type hints or porting from well-known APIs to another one (e.g. from <code>requests</code> to <code>urllib</code> to get rid of external dependencies).</p>
<p>You also want to remove any ambiguity in the prompt, as the LLM then needs to make an assumption what you meant.
For example, this prompt is ambiguous:</p>
<pre class="codehilite"><code>Generate a GUI with a colored background and a big button in the center. 
It should be green.
</code></pre>

<p>In this prompt, the <em>coreference</em> "it" is ambiguous, as it could refer to either the background or the button.
A better prompt would be to resolve this reference, removing the ambiguity and making the prompt clearer:</p>
<pre class="codehilite"><code>Generate a GUI with a colored background and a big button in the center. 
The button should be green.
</code></pre>

<h3 id="formatting">Formatting</h3>
<p>Another crucial technique is the formatting of prompts.  In practice, you want to format your prompts close to StackOverflow questions and GitHub discussion, i.e., written in English, with the content being formatted as markdown, with code being inside triple backticks and <strong>the context/code being first, followed by the question</strong>.  Use headlines (<code># Headline</code>) to show the model what is of importance and add structure to the prompt.  When prompting Claude, <a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags">use XML tags</a> to structure your prompt.  You can also mix XML tags with markdown formatting, for example like this:</p>
<pre class="codehilite"><code>I have this polars code which I want to rewrite using the Lazy API:
```python
(
    df
    .read_csv(&quot;data.csv&quot;)
    .select(  
            [  
                pl.col(&quot;Name&quot;).str.to_lowercase(),  
                pl.col(&quot;Age&quot;).round(2)  
            ]  
        )
)
```

Here are the docs for the Lazy API:
&lt;docs&gt;
# Usage

With the lazy API, Polars doesn't run each query line-by-line but instead processes the full query end-to-end. 
To get the most out of Polars it is important that you use the lazy API because:

- the lazy API allows Polars to apply automatic query optimization with the query optimizer
- the lazy API allows you to work with larger than memory datasets using streaming
- the lazy API can catch schema errors before processing the data

Here we see how to use the lazy API starting from either a file or an existing DataFrame.
[Rest of the docs]
&lt;/docs&gt;

Rewrite the code into the lazy style given the docs.
</code></pre>

<h3 id="limit-the-size-of-your-prompt">Limit the size of your prompt</h3>
<p>While context lengths are improving continuously, they still struggle to effectively use the whole context. Therefore, you want to limit the information you use as input in your prompts. Avoid pasting a whole log file, the complete code base or the giant stack trace and instead focus on the relevant parts to your problem. That also applies to your chat session: The longer it goes on, the less accurate the model becomes. If the model is stuck with an error, you should start a new chat, summarize the problem and start from scratch. I do this after the model produces two errors in a row, i.e., when my chat looks like this:</p>
<pre class="codehilite"><code>Me: &lt;Initial Prompt&gt;
LLM: &lt;Code with Bug&gt;
Me: &lt;Error/Stack Trace&gt;
LLM: &lt;&quot;Fixed&quot; Code with the same Bug&gt;
Me: &lt;Error/Stack Trace&gt;
LLM: &lt;&quot;Fixed&quot; Code with the same Bug&gt;
</code></pre>

<p>At this point, I'd start a new chat session with either the initial prompt.</p>
<h3 id="lrms-need-to-be-prompted-different">LRMs need to be prompted different</h3>
<p>Large Reasoning Models (LRMs) like o1 work differently from other LLMs, therefore they need to be prompted differently. They use more compute resources after you have typed your prompt to come up with an answer. Therefore, you need to wait longer for a response by the model. You still want to be specific in your prompt, but you want to avoid specifying <em>how</em> the model should tackle the prompting. So while this is crucial for "normal" LLMs as mentioned previously, it actually <a href="https://leehanchung.github.io/blogs/2024/10/08/reasoning-understanding-o1/">hurts the performance of o1</a>.</p>
<p>In my experience, o1 is not as strong a coder as Claude, but it can tackle coding problems which need a high-level overview and a deep dive into the problem. This sounds rather vague, so let me give you an example:</p>
<p>We have a small GUI which displays some information in a panel and is also able to set some values. This GUI is written in Tkinter and is around 600 LOC pure Python. o1 was able to rewrite the entire GUI into a streamlit application after some back and forth, with the end result improving upon the original GUI. All other LLMs, including 3.5 Sonnet, failed in various ways, even when pouring a lot of time into prompting and debugging.</p>
<p>However, o1 made some minor coding mistakes and also shares the knowledge cutoff with 4o of October 2023, so I needed to fix some small bugs manually or together with Claude. As a rule of thumb: When you give o1 a problem, and you need to wait &gt;40s for a response, the problem is probably more suited for o1. Everything else is probably better suited for Claude. In my experience, LLMs like Claude can generate 300-500 LOC code with few to zero bugs from scratch, which is plenty for <em>a lot of scripts</em>, while o1 is able to generate 800-1000 LOC code within a chat session.</p>
<p>Another area where I prefer o1 over other LLMs is the explanation of code. This is purely subjective, but I prefer the deep dives and high-level overviews of o1 over explanations by other models. Again the knowledge cutoff of o1 will bite you if you work with cutting-edge libraries and you need to fall back to other models.</p>
<p>Overall, o1 is a very <em>strange</em> model, which is harder to use than other LLMs in my experience. Therefore, you need to play around with the model even more to get a feeling when to use it and how to prompt it properly.</p>
<p>Right now, we are in the <em>GPT-3</em> era of reasoning models in my opinion: A powerful model which works for a handful of domains, but is hard to use and remains niche for most people. This <em>could</em> change with the release of the full o1, but it remains to be seen.</p>
<h2 id="final-remarks">Final Remarks</h2>
<p>Like every other skill, you can only improve in using LLMs by actually using them and developing an intuition how to use them and their limitations. While doing this, I encourage you to collect prompts and problems which LLMs were unable to solve in a text file. When a new LLM releases, use these prompts to assess whether the new LLM improves on problems you care about.
Since the release of 3.5 Sonnet, I found myself writing a lot of simple scripts for automating things, be it python or small browser extensions in the form of Userscripts. Normally, these scripts would be a weekend project, but with LLMs, I finish them in an evening. I wouldn't want to miss them anymore.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:jq">
<p>See <a href="https://github.com/simonw/llm-jq">llm-jq</a> for generating jq syntax&#160;<a class="footnote-backref" href="#fnref:jq" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:gemini">
<p>If you use the easy-to-use API from <a href="https://aistudio.google.com">Google AI Studio</a>, you can use the API for free, but the data from the <a href="https://ai.google.dev/pricing">free usage is used to train the models</a>.&#160;<a class="footnote-backref" href="#fnref:gemini" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:deepseek">
<p>The newer DeepSeek v2.5 shows some regressions against the previous version.&#160;<a class="footnote-backref" href="#fnref:deepseek" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:system-prompt">
<p>The system prompt for Claude can be accessed <a href="https://docs.anthropic.com/en/release-notes/system-prompts#oct-22nd-2024">here</a>&#160;<a class="footnote-backref" href="#fnref:system-prompt" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:espanso">
<p>For this, a text expander like the inbuilt functionality in macOS or <a href="https://espanso.org/">espanso</a> for every OS comes in handy. Save some of your most-used prompts with text shortcuts and re-use them easily.&#160;<a class="footnote-backref" href="#fnref:espanso" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
</ol>
</div>]]></content>
    <link href="https://florianbrand.de/posts/llm-programmer-tutorial"/>
    <summary>An overview of LLMs and prompting techniques for coders</summary>
    <category term="llms"/>
    <category term="prompting"/>
    <category term="applied ai"/>
    <contributor>
      <name>Florian Brand</name>
      <email>privat@florianbrand.de</email>
    </contributor>
    <published>2024-11-04T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://florianbrand.de/posts/odr</id>
    <title>Using OpenAI's Deep Research to save Time and Money</title>
    <updated>2025-03-09T21:30:23.543965+00:00</updated>
    <author>
      <name>Florian Brand</name>
      <email>privat@florianbrand.de</email>
    </author>
    <content type="CDATA"><![CDATA[<h1 id="using-openais-deep-research-to-save-time-and-money">Using OpenAI's Deep Research to save Time and Money</h1>
<p><a href="https://openai.com/index/introducing-deep-research/">OpenAI's Deep Research</a> (ODR), which was released a bit over a month ago, has become a valuable tool for me.
Initially, I wanted to compare it against the countless competitors, but none of them<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> come close to the quality of OpenAI's offering; most of them are either shallow listicle-fests or contain countless mistakes.</p>
<h2 id="the-versatility-of-odr">The versatility of ODR</h2>
<p>My most common use case is to use it as a research assistant to get into a new topic quickly.
This is the most obvious and most advertised use case for Deep Research, which will <a href="https://www.interconnects.ai/p/deep-research-information-vs-insight-in-science">only increase in quality and usage over time</a>.</p>
<p>At least right now, the outputs cannot replace a full literature review, but they are good enough to get a good intro into a new topic, spanning around 3–6 papers usually, which saves around a day of finding and reading relevant papers.
Aside from the summaries of those papers, ODR can also compare the approaches and findings and transfer them onto other domains.</p>
<p>Apart from that, ODR is also good for finding fixes for obscure bugs and niche libraries when tasked to scout GitHub issues and obscure StackOverflow posts for the solution. The underlying model behind ODR is trained to generate reports, so it will not fix the code for you, but instead give you a detailed rundown of the possible solutions.</p>
<p>A totally different use case, which I've not seen mentioned elsewhere, is using it as a shopping assistant for extremely niche products. As an example, I needed a heavy-duty rack which very specific and non-standard measurements to fit in a recess of my apartment. I have to admit, I was unable to find anything myself with some hours of searching, whereas ODR was able to find the single option satisfying the given constraints.</p>
<p>ODR is also capable of finding the cheapest option for a given product, which not even Idealo, the biggest price comparison website in Germany, could do. It is also capable of finding valid replacement parts for a given product when supplied with the product's name, something I expected competitors (like Perplexity) to accomplish, but they could not.</p>
<h2 id="getting-the-most-out-of-odr">Getting the most out of ODR</h2>
<p>That said, ODR is not perfect and not as straightforward to use as other, LLM-based tools, despite the seeming simplicity of clicking the "Deep Research" button in ChatGPT and then typing in your prompt and answering the follow-up questions. One thing which isn't obvious: The model chosen when clicking the button doesn't matter, Deep Research will always use o3 behind the scenes, even when the selected model is 4o-mini.</p>
<p>The most important aspect of using ODR is that <strong>prompting matters (again)</strong>. Whereas other products and LLMs these days are good enough for the majority of prompts, the difference in the outputs of ODR is night and day depending on the prompt.</p>
<p>Good prompts are highly specific and detailed, describing the goal, possible constraints and the desired output format. A viable approach is to use LLMs to generate a prompt for ODR, I've been using <a href="https://www.florianbrand.de/posts/odr-prompt">this prompt template</a> from <a href="https://x.com/buccocapital/status/1890745551995424987">this tweet</a> with o1-pro to generate a prompt for ODR. In my tests, using this template compared to prompting directly resulted in &gt;40% longer reports, which go into more detail and are more structured.</p>
<p>Good prompts also specify the (sub)set of websites to use for the research. The default set of websites ODR chooses is a bit better than the usual, SEO-sloptimized first page of Google results, but this is far from perfect. </p>
<p>As examples: Adding to the prompt that the research should only use ArXiv (and similar sites) leads to better results for literature reviews; asking it to only use primary sources from NVIDIA leads to a correct comparison of GPU specs; asking it to use only Chinese-language sites like Weixin gives you better insights into the Chinese community than any third-party English-language site could give. </p>
<h3 id="limitations">Limitations</h3>
<p>When using ODR for comparisons, e.g. to compare different papers or models, I found that the limit seems to be 2-3 comparisons max, i.e., comparing two models (like Qwen against GPT-4o) is fine, but adding more models to the comparison leads to numerous mistakes. I found this to be also true for other tasks: When you prompt it for too many things at once, the output will degrade quickly.</p>
<p>Aside from that, ODR has some technical limitations. It cannot access gated content, which will <a href="https://stratechery.com/2025/deep-research-and-knowledge-value/">become an increasing part of the internet in the near future</a>. It also cannot access YouTube nor the transcripts of YouTube videos. I also was unable to prompt it to use a transcription service to get the content of videos. It can, however, read PDFs, access images and execute Python code, although I haven't seen that for my queries yet. </p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>And I've tried quite a few, including some open-source projects, <a href="https://gemini.google/overview/deep-research/?hl=en">Gemini Deep Research</a>, <a href="https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research">Perplexity Deep Research</a>, <a href="https://x.ai/blog/grok-3">Grok Deep Search</a>, <a href="https://you.com/ari">You ARI</a>, even the recently released and hyped <a href="https://manus.im/">Manus</a>.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>]]></content>
    <link href="https://florianbrand.de/posts/odr"/>
    <summary>An overview and guide on how to use OpenAI's Deep Research (ODR)</summary>
    <category term="chatgpt"/>
    <category term="llms"/>
    <contributor>
      <name>Florian Brand</name>
      <email>privat@florianbrand.de</email>
    </contributor>
    <published>2025-03-09T00:00:00+00:00</published>
  </entry>
</feed>
